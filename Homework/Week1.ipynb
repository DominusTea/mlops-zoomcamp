{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7216635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623b998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e6dbb",
   "metadata": {},
   "source": [
    "### Q1: Reading January's data for yellow taxis trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b328dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns is: 19\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/yellow_tripdata_2022-01.parquet')\n",
    "num_cols = len(df.columns)\n",
    "print(f\"The number of columns is: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4a17e",
   "metadata": {},
   "source": [
    "### Q2. Computing duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3eccd3",
   "metadata": {},
   "source": [
    "First we look at the columns found in our data and identify the columns that correspond to the trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b422df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.PULocationID.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c081dd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'airport_fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f429ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -1 days +23:42:11\n",
       "1         -1 days +23:51:36\n",
       "2         -1 days +23:51:02\n",
       "3         -1 days +23:49:58\n",
       "4         -1 days +23:22:28\n",
       "                 ...       \n",
       "2463926   -1 days +23:54:02\n",
       "2463927   -1 days +23:49:21\n",
       "2463928   -1 days +23:49:00\n",
       "2463929   -1 days +23:47:57\n",
       "2463930   -1 days +23:33:00\n",
       "Length: 2463931, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['tpep_pickup_datetime'] - df['tpep_dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072aa86",
   "metadata": {},
   "source": [
    "The trip duration can be calculated by subtracting the dropoff time from the pickup time, which correspond to the \"tpep_pickup_datetime\" and \"tpep_dropoff_datetime\" columns accordingly.\n",
    "\n",
    "Subtracting two datetime (<M8[ns]) columns in pandas will result in a column of type timedelta64[ns]. We can then apply a lambda function over the column (which is a Series object in the Pandas library) to find the trip's duration in seconds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a597f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "df['duration'] = df.duration.apply(lambda td: td.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6374526",
   "metadata": {},
   "source": [
    "Having defined and populated the 'duration' column, we can perform numeric operations over it. \n",
    "In order to calculated the standard deviation of the trips duration we use Pandas' built-in std function for Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc800d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The std is:  46.44530513776802\n"
     ]
    }
   ],
   "source": [
    "print(\"The std is: \", df['duration'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee372d",
   "metadata": {},
   "source": [
    "### Q3. Dropping outliers\n",
    "\n",
    "We then attempt to remove the outliers in our dataset. Using Panda's DataFrame API we can filter out all dataset instances where the trip's duration is over 60 minutes or under 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d102564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "outliers_percentage = (len(df.index) - len(df_filtered.index))/len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96463e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers correspond to 1.72452 of our dataset\n",
      "Dataset size after removing outliers:  2421440\n"
     ]
    }
   ],
   "source": [
    "print(f\"Outliers correspond to {round(100*outliers_percentage, 5)} of our dataset\")\n",
    "print(\"Dataset size after removing outliers: \", len(df_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37612cf",
   "metadata": {},
   "source": [
    "### Q4. One-hot encoding\n",
    "\n",
    "Our data contains categorical data. In order to transform them into something that a Machine Learning algorithm can understand we will use sklearn's Dict Vectorizer, which transform the categorical columns into their one-hot encoded equivalents.\n",
    "\n",
    "*Note that we cannot simply transform the categorical columns by assigning them into an integer that corresponds to each row's categorical value's index from a dictionary of all possible values. That is because, in doing so, we would introduce implicit bias in our models, since categorical values that correspond neighbouring index values in the categorical value dictionary would be expected to be semantically similary by a common loss function such as MSE*\n",
    "\n",
    "Furthermore, our encoded data will contain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36f998",
   "metadata": {},
   "source": [
    "First we decide which categorical columns will be used by our model. Pickup and dropoff location seem suitable for a trip prediction model.\n",
    "However these columns contain data of type string. Therefore, we will have to transform these columns to type string before using our one-hot encoder vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301896ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical_columns = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "# Numerical_columns = ['trip_distance']\n",
    "\n",
    "# df_filtered[Categorical_columns] = df_filtered[Categorical_columns].astype(str)\n",
    "# df_condensed = pd.concat([df_filtered[Categorical_columns].astype(str), df_filtered[Numerical_columns]])\n",
    "df_condensed = df_filtered[Categorical_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c688fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(df_condensed.to_dict(orient='records'))\n",
    "y_train = df_filtered['duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe8dfd",
   "metadata": {},
   "source": [
    "We can see that the resulting matrix X_train has dimensions equal to the number of records in our dataset (after outlier filtering) times the sum of the unique categorical values in each categorical variable column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d83e6985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of One-hot encoded Matrix: (2421440, 2)\n",
      "Number of unique PULocationID' and 'DOLocationID combinations: 515\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of One-hot encoded Matrix: {X_train.shape}\")\n",
    "print(f\"Number of unique PULocationID' and 'DOLocationID combinations: {len(set(df_condensed['DOLocationID'].astype(str)))+len(set(df_condensed['PULocationID'].astype(str)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d03877",
   "metadata": {},
   "source": [
    "### Q5. Training a model\n",
    "\n",
    "Now, we utilize the feature matrix X_train in order to train a Linear Regression model. In order to train the model we will have to utilize the y_train as the target variable, that is to say the variable which our model must learn to predict correctly using the X_train feature matrix as variable.\n",
    "\n",
    "For the Linear Regression model, we use Sklearn's LinearRegression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e4cd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05352bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc27e07",
   "metadata": {},
   "source": [
    "We then calculate the training loss using the MSE metric, again utilizing the sklearn's library implementation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "710875de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  8.920327827581444\n"
     ]
    }
   ],
   "source": [
    "print(\"Training MSE: \", mean_squared_error(y_train, y_pred_train, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b375f0",
   "metadata": {},
   "source": [
    "### Q6 Evaluation\n",
    "\n",
    "Similar to Q5 we can use the linear regression on the validation subset of our dataset. This way we can evaluate the Linear Regressor's results on data that weren't used in training it. \n",
    "\n",
    "In order to find the feature matrix for the validation subset we have to repeat every step that we followed for the train subset. Therefore, we incorporate every such step in the \"read_dataframe\" function and repeat them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a23ba324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6102b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_dataframe('../data/yellow_tripdata_2022-01.parquet')\n",
    "df_val = read_dataframe('../data/yellow_tripdata_2022-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4668e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset size:  2421440\n",
      "Validation subset size:  2918187\n"
     ]
    }
   ],
   "source": [
    "print(\"Training subset size: \", len(df_train))\n",
    "print(\"Validation subset size: \", len(df_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea70518",
   "metadata": {},
   "source": [
    "We can then, similarly, design a \"calc_feature_matrix\" which will calculate each dataframe's feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "258b531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feature_matrix(df_train, df_val, cols=['PULocationID', 'DOLocationID']):\n",
    "    dv = DictVectorizer()\n",
    "    train_dicts = df_train[cols].to_dict(orient='records')\n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "    val_dicts = df_val[cols].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts)\n",
    "    \n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a52e3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = calc_feature_matrix(df_train, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbf25679",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['duration'].values\n",
    "y_val = df_val['duration'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a968f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.786389417027388"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1f05c",
   "metadata": {},
   "source": [
    "*Note that the RMSE loss in the validation subset is higher than the RMSE loss in the training subset. This is to expected since the linear regressor was trained to minimize the loss on the train subset and not the validation subset.* \n",
    "\n",
    "*However, the linear regression model we trained generalizes well to the validation subset, since it achieves an RMSE error that is pretty close to the training subset one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f893e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dipl2] *",
   "language": "python",
   "name": "conda-env-dipl2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
